# 训练模型

```commandline
python3 training_code/train_tfidf.py
python3 training_code/train_bert.py
```

# 压测服务

```commandline
cd test/

ab -n 100 -c 100 -p data.json -T 'application/json' -H 'accept: application/json' 'http://0.0.0.0:8000/v1/text-cls/regex'
ab -n 100 -c 100 -p data.json -T 'application/json' -H 'accept: application/json' 'http://0.0.0.0:8000/v1/text-cls/tfidf'
ab -n 100 -c 100 -p data.json -T 'application/json' -H 'accept: application/json' 'http://0.0.0.0:8000/v1/text-cls/bert'
```

# 接口

```commandline
curl -X 'POST' \
  'http://0.0.0.0:8000/v1/text-cls/tfidf' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "request_id": "string",
  "request_text": "帮我播放周杰伦的歌曲"
}'
```

# 部署

```commandline
fastapi run main.py
```
# 多模型文本分类系统：实验报告与项目文档

## 1. 项目概述
本项目构建了一个多路由的自然语言理解（NLU）服务，旨在对比分析不同技术路线在文本意图识别任务上的表现。项目集成了四种主流的文本分类方案：正则表达式 (Regex)、TF-IDF + 机器学习、FastText以及BERT (预训练模型)。通过 FastAPI 框架对外提供统一接口，并针对不同模型进行了并发压力测试（AB Test），以评估其在生产环境下的精度与性能平衡。

---
## 2. 模型优缺点深度对比分析
本章节详细阐述了四种模型的技术特性、优缺点及适用场景。

## 2.1 核心维度对比矩
| 维度 | Regex (正则) | TF-IDF (统计) | FastText (浅层网络) | BERT (深度语义) |
| 核心原理 | 字符串模式硬匹配 | 词频-逆文档频率统计 | 字符级 N-gram 嵌入 | Transformer 自注意力机制 |
| 训练成本 | 无 (需人工编写) | 极低 (CPU 分钟级) | 低 (CPU 分钟级) | 高 (需 GPU 数小时) |
| 推理延迟 | 极低| 低 | 低 | 高  |
| 语义理解 | 无 | 弱 (基于关键词重合) | 中 (捕捉局部词序) | 极强 (理解上下文/多义词)|
| 泛化能力 | 差 (规则外即失效) | 一般 | 较好 (抗错别字) | 极强 (支持模糊表达) |

### 2.2 模型详细剖析

## A. Regex (正则表达式)

优点：
确定性：对于“播放/暂停”这类绝对指令，不会产生算法波动。
调试直观：匹配不上直接查正则式，不需要重新训练模型。

缺点：
泛化“死穴”：比如正则写了“播放.*歌曲”，用户说“我想听周杰伦”，则会完全失效。
维护黑洞：随着业务增加，几百条正则互相冲突（Shadowing）是开发者的噩梦。

## B. TF-IDF + 机器学习
优点：
关键特征突出：能自动识别出文本中信息量最大的词（如“周杰伦”比“帮我”重要）。
计算透明：可以通过权重矩阵清晰看到哪些词导致了分类结果。

缺点：
“词袋”缺陷：无法区分“我欠你钱”和“你欠我钱”（词汇一样但词序不同）。
维度灾难：词表过大时，特征矩阵极其稀疏，影响性能。

## C. FastText

优点：
极致性能：在处理大规模分类（如万级分类）时，速度比 BERT 快几个数量级。
抗噪性：通过子词（Subword）信息，能够处理部分拼写错误或 OOV（未登录词）问题。

缺点：
结构简单：无法捕捉长距离的依赖关系。

## D. BERT

优点：
多义词处理：能根据上下文分辨“苹果”是水果还是手机。
迁移学习：即便你的项目数据很少，利用通用领域的预训练模型也能达到很高精度。

缺点：
上线门槛高：需要 GPU 显存，推理耗时长（通常在 50ms-200ms），不适合超高并发。
黑盒属性：分类错误时，很难通过调整参数来快速“修复”某个特定 case。
---

## 3. 项目代码实现

### 3.1 核心服务代码 (`main.py`)
基于 FastAPI 实现的后端服务，包含四个模型的独立路由。

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import time
# 实际项目中需导入加载模型的库，如 joblib, torch, transformers
# import joblib
# import torch

app = FastAPI(title="Text Classification Service Benchmark")

# 定义请求体结构
class ClassificationRequest(BaseModel):
    request_id: str
    request_text: str

# --- 模拟模型加载 (占位符) ---
class MockModelEngine:
    """
    为了演示代码结构，此处使用 Mock 类。
    实际部署时需在 startup 事件中加载真实模型文件。
    """
    def predict_regex(self, text):
        # 简单规则示例
        if "播放" in text or "歌曲" in text:
            return "music_intent"
        return "unknown"

    def predict_tfidf(self, text):
        # 模拟 sklearn predict
        return "video_intent"

    def predict_bert(self, text):
        # 模拟 transformer 推理耗时
        # time.sleep(0.05) # 模拟 50ms 延迟
        return "chat_intent"

engine = MockModelEngine()

# --- 路由定义 ---

@app.post("/v1/text-cls/regex")
async def cls_regex(request: ClassificationRequest):
    """正则表达式匹配路由：极速、硬匹配"""
    start_time = time.time()
    label = engine.predict_regex(request.request_text)
    return {
        "id": request.request_id,
        "label": label,
        "model": "regex",
        "latency_ms": (time.time() - start_time) * 1000
    }

@app.post("/v1/text-cls/tfidf")
async def cls_tfidf(request: ClassificationRequest):
    """TF-IDF + ML 路由：统计特征"""
    start_time = time.time()
    label = engine.predict_tfidf(request.request_text)
    return {
        "id": request.request_id,
        "label": label,
        "model": "tfidf",
        "latency_ms": (time.time() - start_time) * 1000
    }

@app.post("/v1/text-cls/bert")
async def cls_bert(request: ClassificationRequest):
    """BERT 路由：深度语义理解"""
    start_time = time.time()
    label = engine.predict_bert(request.request_text)
    return {
        "id": request.request_id,
        "label": label,
        "model": "bert",
        "latency_ms": (time.time() - start_time) * 1000
    }

@app.get("/health")
async def health_check():
    return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)