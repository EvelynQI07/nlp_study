import pandas as pd
import random
import os

# 1. å®šä¹‰æ•°æ®ç”Ÿæˆè§„åˆ™
data_samples = {
    "ç§‘æŠ€": [
        "è‹¹æœå‘å¸ƒiPhone17ï¼Œæ­è½½A17èŠ¯ç‰‡æ€§èƒ½çˆ†ç‚¸", "åä¸ºMate60 Proéº’éºŸèŠ¯ç‰‡å›å½’å¼•å‘çƒ­è®®",
        "OpenAIå‘å¸ƒGPT-4ï¼Œäººå·¥æ™ºèƒ½å†ä¸Šæ–°å°é˜¶", "è‹±ä¼Ÿè¾¾æ˜¾å¡ä»·æ ¼æŒç»­ä¸Šæ¶¨ï¼Œç®—åŠ›éœ€æ±‚æ—ºç››",
        "é©¬æ–¯å…‹SpaceXæ˜Ÿèˆ°å‘å°„å¤±è´¥ï¼Œä½†å–å¾—éƒ¨åˆ†æ•°æ®", "å°ç±³æ±½è½¦æœ€æ–°è°ç…§æ›å…‰ï¼Œé›·å†›äº²è‡ªè¯•é©¾",
        "å¾®è½¯Copilotå…¨é¢æ¥å…¥Windowsç³»ç»Ÿ", "åŠå¯¼ä½“è¡Œä¸šè¿æ¥å¯’å†¬ï¼Œä¸‰æ˜Ÿåº“å­˜ç§¯å‹ä¸¥é‡"
    ],
    "ä½“è‚²": [
        "æ¹–äººé˜Ÿè©¹å§†æ–¯ç ä¸‹40åˆ†ï¼Œå¸¦é¢†çƒé˜Ÿé€†è½¬", "æ¢…è¥¿è·å¾—ç¬¬å…«åº§é‡‘çƒå¥–ï¼Œå†å²ç¬¬ä¸€äºº",
        "ä¸­å›½å¥³æ’ä¸–ç•Œè”èµ›å‡»è´¥å·´è¥¿ï¼Œæ™‹çº§å†³èµ›", "æ›¼è”ä¸»åœºæƒ¨è´¥ï¼Œä¸»æ•™ç»ƒæ»•å“ˆæ ¼é¢ä¸´ä¸‹è¯¾",
        "F1çº¢ç‰›è½¦é˜Ÿç»´æ–¯å¡”æ½˜æå‰é”å®šå¹´åº¦æ€»å† å†›", "å§šæ˜è¾å»ç¯®åä¸»å¸­èŒåŠ¡ï¼Œå¼•å‘å¤–ç•Œå…³æ³¨",
        "å…¨çº¢å©µè·³æ°´å†ç°æ°´èŠ±æ¶ˆå¤±æœ¯ï¼Œå¤ºå¾—é‡‘ç‰Œ", "Cç½—åœ¨æ²™ç‰¹è”èµ›ä¸Šæ¼”å¸½å­æˆæ³•"
    ],
    "å¨±ä¹": [
        "éœ‰éœ‰æ³°å‹’æ–¯å¨å¤«ç‰¹æ¼”å”±ä¼šé—¨ç¥¨ç§’ç©º", "è¯ºå…°æ–°ç‰‡ã€Šå¥¥æœ¬æµ·é»˜ã€‹æ¨ªæ‰«å¥¥æ–¯å¡",
        "å‘¨æ°ä¼¦æ–°ä¸“è¾‘å‘å¸ƒï¼ŒæœåŠ¡å™¨ä¸€åº¦å´©æºƒ", "æŸé¡¶æµæ˜æ˜Ÿå¡Œæˆ¿ï¼Œå“ç‰Œæ–¹ç´§æ€¥è§£çº¦",
        "æ˜¥èŠ‚æ¡£ç”µå½±ç¥¨æˆ¿çªç ´100äº¿ï¼Œè´¾ç²æ–°ç‰‡é¢†è·‘", "BLACKPINKç»­çº¦å­˜ç–‘ï¼Œè‚¡ä»·åº”å£°ä¸‹è·Œ",
        "å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡æ­æ™“ï¼Œå†·é—¨ä½³ä½œçˆ†å†·è·å¥–", "æµæµªåœ°çƒ3å®£å¸ƒå®šæ¡£ï¼Œå´äº¬å›å½’ä¸»æ¼”"
    ],
    "è´¢ç»": [
        "ç¾è”å‚¨å®£å¸ƒåŠ æ¯25ä¸ªåŸºç‚¹ï¼Œç¾è‚¡ä¸‰å¤§æŒ‡æ•°ä¸‹è·Œ", "è´µå·èŒ…å°è‚¡ä»·åˆ›å†å²æ–°é«˜ï¼Œåˆ†çº¢æ–¹æ¡ˆå…¬å¸ƒ",
        "å›½é™…é‡‘ä»·æŒç»­èµ°é«˜ï¼Œå¤§å¦ˆæ’é˜ŸæŠ¢è´­é»„é‡‘", "æ¯”ç‰¹å¸è·Œç ´3ä¸‡ç¾å…ƒå…³å£ï¼Œå¸åœˆä¸€ç‰‡å“€åš",
        "æ’å¤§åœ°äº§å€ºåŠ¡é‡ç»„å¤±è´¥ï¼Œè®¸å®¶å°è¢«é‡‡å–æªæ–½", "Aè‚¡å†æ¬¡æ‰“å“3000ç‚¹ä¿å«æˆ˜",
        "CPIæ•°æ®å‡ºç‚‰ï¼Œé€šèƒ€å‹åŠ›ä¾ç„¶å­˜åœ¨", "å·´è²ç‰¹å‡æŒæ¯”äºšè¿ªè‚¡ä»½ï¼Œå¥—ç°æ•°äº¿æ¸¯å…ƒ"
    ]
}

# 2. ç”Ÿæˆ CSV æ–‡ä»¶
rows = []
# ç”Ÿæˆ 200 æ¡æ•°æ® (é€šè¿‡éšæœºé‡å¤é‡‡æ ·æ¨¡æ‹Ÿ)
for _ in range(200):
    label = random.choice(list(data_samples.keys()))
    text = random.choice(data_samples[label])
    rows.append([text, label])

# ä¿å­˜ä¸º dataset.csvï¼Œä½¿ç”¨åˆ¶è¡¨ç¬¦ \t åˆ†éš”ï¼Œæ— è¡¨å¤´
df = pd.DataFrame(rows)
df.to_csv("dataset.csv", sep="\t", header=False, index=False)

print(f"âœ… æ•°æ®é›†å·²ç”Ÿæˆï¼šdataset.csvï¼Œå…± {len(df)} æ¡æ•°æ®")
print("å‰5æ¡æ•°æ®é¢„è§ˆï¼š")
print(df.head())

import pandas as pd
import torch
import numpy as np  # è¡¥å……ç¼ºå¤±çš„ numpy
import os
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertForSequenceClassification, AdamW

# -------------------------- 1. æ•°æ®å‡†å¤‡ --------------------------
print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
# åŠ è½½åˆšæ‰ç”Ÿæˆçš„ dataset.csv
dataset = pd.read_csv("dataset.csv", sep="\t", header=None)
dataset.columns = ["text", "label"]  # ç»™åˆ—èµ·ä¸ªåå­—æ–¹ä¾¿æ“ä½œ

# åˆå§‹åŒ–å¹¶æ‹Ÿåˆæ ‡ç­¾ç¼–ç å™¨
lbl = LabelEncoder()
dataset['label_id'] = lbl.fit_transform(dataset['label'])

# æ‰“å°ç±»åˆ«æ˜ å°„å…³ç³»ï¼Œæ–¹ä¾¿åç»­éªŒè¯
label_map = {index: label for index, label in enumerate(lbl.classes_)}
print(f"ç±»åˆ«æ˜ å°„: {label_map}")

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
x_train, x_test, y_train, y_test = train_test_split(
    dataset['text'].values,
    dataset['label_id'].values,
    test_size=0.2,
    stratify=dataset['label_id'].values,
    random_state=42
)

# -------------------------- æ¨¡å‹è·¯å¾„é…ç½® --------------------------
# ä¼˜å…ˆå°è¯•ä½¿ç”¨ HuggingFace åœ¨çº¿æ¨¡å‹ï¼Œå¦‚æœæƒ³ç”¨æœ¬åœ°æ¨¡å‹ï¼Œè¯·ä¿®æ”¹ model_path
model_path = 'bert-base-chinese'

print(f"æ­£åœ¨åŠ è½½ BERT æ¨¡å‹: {model_path} ...")
try:
    tokenizer = BertTokenizer.from_pretrained(model_path)
    # è‡ªåŠ¨è®¡ç®—ç±»åˆ«æ•°é‡ï¼šlen(lbl.classes_) åº”è¯¥æ˜¯ 4
    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lbl.classes_))
    print("ğŸš€ æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è·¯å¾„ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    exit()

# ç¼–ç æ•°æ®
train_encoding = tokenizer(list(x_train), truncation=True, padding=True, max_length=64)
test_encoding = tokenizer(list(x_test), truncation=True, padding=True, max_length=64)


# -------------------------- 2. æ•°æ®é›†å’ŒåŠ è½½å™¨ --------------------------
class NewsDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(int(self.labels[idx]))
        return item

    def __len__(self):
        return len(self.labels)


train_dataset = NewsDataset(train_encoding, y_train)
test_dataset = NewsDataset(test_encoding, y_test)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# -------------------------- 3. è®­ç»ƒé…ç½® --------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
optimizer = AdamW(model.parameters(), lr=2e-5)


# ç²¾åº¦è®¡ç®—å‡½æ•°
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)


# -------------------------- 4. è®­ç»ƒä¸éªŒè¯é€»è¾‘ --------------------------
def train(epoch):
    model.train()
    total_train_loss = 0

    for step, batch in enumerate(train_loader):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_train_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        if step % 2 == 0 and step > 0:
            print(f"  Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}")

    avg_train_loss = total_train_loss / len(train_loader)
    print(f"Epoch {epoch} å®Œæˆ | å¹³å‡ Loss: {avg_train_loss:.4f}")


def validation():
    model.eval()
    total_eval_accuracy = 0
    total_eval_loss = 0

    for batch in test_dataloader:
        with torch.no_grad():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

        loss = outputs.loss
        logits = outputs.logits

        total_eval_loss += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = labels.to('cpu').numpy()
        total_eval_accuracy += flat_accuracy(logits, label_ids)

    print(f"âœ… éªŒè¯é›†å‡†ç¡®ç‡: {total_eval_accuracy / len(test_dataloader):.4f}")
    print("-" * 30)


# -------------------------- 5. å¼€å§‹è®­ç»ƒ --------------------------
epochs = 3
for epoch in range(epochs):
    train(epoch)
    validation()

print("è®­ç»ƒç»“æŸï¼")


# -------------------------- 6. é¢„æµ‹æ–°æ ·æœ¬ --------------------------
def predict_sentence(text):
    # 1. å¤„ç†æ–‡æœ¬
    model.eval()
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=64)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # 2. æ¨¡å‹æ¨ç†
        outputs = model(**inputs)
        logits = outputs.logits

        # 3. è·å–æœ€å¤§æ¦‚ç‡çš„ç´¢å¼•
        pred_id = torch.argmax(logits, dim=1).item()

        # 4. è½¬æ¢å›æ–‡å­—æ ‡ç­¾
        pred_label = label_map[pred_id]
        return pred_label


print("\n========== æœ€ç»ˆæµ‹è¯• ==========")
test_sentences = [
    "OpenAIå‘å¸ƒäº†æœ€æ–°çš„Soraå¤§æ¨¡å‹ï¼Œè§†é¢‘ç”Ÿæˆæ•ˆæœæƒŠäºº",  # é¢„æœŸï¼šç§‘æŠ€
    "ä»Šå¤©çš„Aè‚¡ç®€ç›´æ²¡æ³•çœ‹ï¼Œåˆè·Œç ´äº†3000ç‚¹",  # é¢„æœŸï¼šè´¢ç»
    "æ¹–äººé˜Ÿä»Šå¤©åŠ æ—¶èµ›ç»æ€å¯¹æ‰‹",  # é¢„æœŸï¼šä½“è‚²
    "é‚£éƒ¨æ–°ä¸Šæ˜ çš„ç”µå½±ç¥¨æˆ¿å·²ç»ç ´äº¿äº†"  # é¢„æœŸï¼šå¨±ä¹
]

for text in test_sentences:
    result = predict_sentence(text)
    print(f"æ–‡æœ¬: {text}")
    print(f"é¢„æµ‹: ã€{result}ã€‘\n")
